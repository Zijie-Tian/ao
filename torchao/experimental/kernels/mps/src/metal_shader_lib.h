/**
 * This file is generated by gen_metal_shader_lib.py
 */

#ifdef USE_ATEN
using at::native::mps::MetalShaderLibrary;
#else
#include <torchao/experimental/kernels/mps/src/MetalShaderLibrary.h>
#endif

static MetalShaderLibrary metal_lowbit_quantized_lib(R"METAL_LOWBIT(

/**
 * Contents of common.metal
 */

template <typename T> struct Vec4Type {};

template <> struct Vec4Type<float> {
  using type = float4;
};

template <> struct Vec4Type<half> {
  using type = half4;
};

#if __METAL_VERSION__ >= 310
template <> struct Vec4Type<bfloat> {
  using type = bfloat4;
};
#endif



/**
 * Contents of int1mm.metal
 */

#include <metal_stdlib>
using namespace metal;

/**
 * 1-Bit Quantized Linear.
 *
 * @param[A] M x K input tensor of floating point dtype (Float, Half, BFloat16)
 * @param[B] Packed & quantized weight tensor of uint8 dtype. Expected shape is N x (K / 8)
 * @param[scales] 2D tensor containg the scales for each group. Expected shape is #groups x N
 * @param[zeros] 2D tensor containg the zero points for each group. Expected shape is #groups x N
 * @param[outputData] M x N output tensor of floating point dtype (same as input)
 * @param[sizes] The sizes involved in the order: M, K, N
 *
 * Dispatched threads: N x M x 1
 */
template<typename T, unsigned groupSize>
kernel void int1pack_mm(
    constant T                 * A              [[buffer(0)]],
    constant uchar             * B              [[buffer(1)]],
    constant T                 * scales         [[buffer(2)]],
    constant T                 * zeros          [[buffer(3)]],
    device   T                 * outputData     [[buffer(4)]],
    constant uint3             & sizes          [[buffer(5)]], // M, K, N
    uint2                        thread_index   [[thread_position_in_grid]]) {
    const uint K = sizes.y;
    const uint N = sizes.z;
    const uint m = thread_index.y; // 0..M-1
    const uint n = thread_index.x; // 0..N-1
    const uint32_t k_block = (K + groupSize - 1) / groupSize;
    constant T *A_ptr = A + m * K;
    constant uchar *B_ptr = B + n * K / 8;

    float rc = 0.0;
    uint k = 0;
    for (uint32_t kb = 0; kb < k_block ; kb ++) {
      const float scale = float(scales[kb * N + n]);
      const float zero = float(zeros[kb * N + n]);
      for(uint idx = 0; idx < groupSize && k < K; idx+=8, k+=8) {
        const auto a_val0 = float(A_ptr[k + 0]);
        const auto a_val1 = float(A_ptr[k + 1]);
        const auto a_val2 = float(A_ptr[k + 2]);
        const auto a_val3 = float(A_ptr[k + 3]);
        const auto a_val4 = float(A_ptr[k + 4]);
        const auto a_val5 = float(A_ptr[k + 5]);
        const auto a_val6 = float(A_ptr[k + 6]);
        const auto a_val7 = float(A_ptr[k + 7]);

        uchar b0 = B_ptr[(k / 8)];

        uchar w_val0 = b0 & 0x01;
        uchar w_val1 = (b0 & 0x02) >> 1;
        uchar w_val2 = (b0 & 0x04) >> 2;
        uchar w_val3 = (b0 & 0x08) >> 3;
        uchar w_val4 = (b0 & 0x10) >> 4;
        uchar w_val5 = (b0 & 0x20) >> 5;
        uchar w_val6 = (b0 & 0x40) >> 6;
        uchar w_val7 = (b0 & 0x80) >> 7;

        rc += a_val0 * (scale * float(w_val0) + zero);
        rc += a_val1 * (scale * float(w_val1) + zero);
        rc += a_val2 * (scale * float(w_val2) + zero);
        rc += a_val3 * (scale * float(w_val3) + zero);
        rc += a_val4 * (scale * float(w_val4) + zero);
        rc += a_val5 * (scale * float(w_val5) + zero);
        rc += a_val6 * (scale * float(w_val6) + zero);
        rc += a_val7 * (scale * float(w_val7) + zero);
      }
    }
    outputData[m * N + n] = T(rc);
}

#define INSTANTIATE_INT1MM(DTYPE, GSIZE)                                 \
template                                                                 \
[[host_name("int1pack_mm_" #GSIZE "_" #DTYPE)]]                          \
kernel void int1pack_mm<DTYPE, GSIZE>(                                   \
    constant DTYPE             * A              [[buffer(0)]],           \
    constant uchar             * B              [[buffer(1)]],           \
    constant DTYPE             * scales         [[buffer(2)]],           \
    constant DTYPE             * zeros          [[buffer(3)]],           \
    device   DTYPE             * outputData     [[buffer(4)]],           \
    constant uint3             & sizes          [[buffer(5)]],           \
    uint2                        thread_index [[thread_position_in_grid]])

INSTANTIATE_INT1MM(float, 32);
INSTANTIATE_INT1MM(half, 32);
INSTANTIATE_INT1MM(float, 64);
INSTANTIATE_INT1MM(half, 64);
INSTANTIATE_INT1MM(float, 128);
INSTANTIATE_INT1MM(half, 128);
INSTANTIATE_INT1MM(float, 256);
INSTANTIATE_INT1MM(half, 256);
#if __METAL_VERSION__ >= 310
INSTANTIATE_INT1MM(bfloat, 32);
INSTANTIATE_INT1MM(bfloat, 64);
INSTANTIATE_INT1MM(bfloat, 128);
INSTANTIATE_INT1MM(bfloat, 256);
#endif



/**
 * Contents of int2mm_opt.metal
 */

#include <metal_simdgroup>
#include <metal_stdlib>
using namespace metal;

/*
   This code takes heavy inspiration from MLX:
   https://github.com/ml-explore/mlx/blob/main/mlx/backend/metal/kernels/quantized.h
   Specifically:
     - Multiplying activation by inverse scaling factor to reduce compute
   boundedness
     - Handling zero point by accumulating act in separate sum term. Needed with
   optimization done above. MLX MIT License:
   https://github.com/ml-explore/mlx/blob/main/LICENSE
*/

/*
   @brief This shader implements 2-bit matrix-vector multiplication where A
   matrix is fp16, bfloat or float and B matrix is a 2-bit groupwise-quantized weight
   matrix.
   @param [in] A is activation matrix of size M x K.
   @param [in] B is weight matrix of size M x K. Each byte contains 4 2-bit
   values, along K dim, packed together.
   @param [in] scales_ptr is scales ptr corresponding each
   output channel x groups. These are packed as [num_groups = ceil(K / group_size), N]. N = output
   channels.
   @param [in] zeros_ptr is zero points corresponding each
   output channel x groups. These are packed as [num_groups = ceil(K / group_size), N]. N = output
   channels.
   output channel x groups. These are packed as [num_groups = ceil(K / group_size), N, 2]. N = output
   @param [out] output_data is output matrix of size M x N.
   @param [in] sizes array contains values of M, K and N.
   @param [in] thread_index is global thread id.
   @param [in] tid_in_simdgruop is thread id in simdgroup. e.g. in simdgroup of size 32 it can be in [0-31].
*/
template <typename T, unsigned group_size>
kernel void int2pack_mm(constant T *A [[buffer(0)]],
                        constant uchar *B [[buffer(1)]],
                        constant T *scales_ptr [[buffer(2)]],
                        constant T *zeros_ptr [[buffer(3)]],
                        device T *output_data [[buffer(4)]],
                        constant uint3 &sizes [[buffer(5)]], // M, K, N
                        uint3 thread_index [[thread_position_in_grid]],
                        uint tid_in_simdgroup [[thread_index_in_simdgroup]]) {
  constexpr uint threads_per_channel = 32;
  constexpr uint ks_per_thread = 4;
  constexpr uint k_pack_factor = 4;
  const uint K = sizes.y;
  const uint N = sizes.z;
  uint n = thread_index.x; // 0..N/4-1
  uint m = thread_index.z; // 0..M
  n = n / threads_per_channel;
  n = n * 4;
  // This is starting k for each thread. In the example above, for thread 1 this
  // value will be 4.
  uint k = (tid_in_simdgroup % threads_per_channel) * ks_per_thread;
  constexpr int k_jump = threads_per_channel * ks_per_thread;

  using vecT = typename Vec4Type<T>::type;
  constant vecT *A_ptr = reinterpret_cast<constant vecT *>(A + m * K);
  constant uchar *B_ptr = B + ((n * K) / k_pack_factor);

  thread float4 result = float4(0.0);
  // We multipy group of 4 channels with these scales.
  // Because corresponding values from weight matrix are effectively left
  // shifted. This is to avoid doing right shift on those values which ends up
  // affecting performance. This is the trick applied in MLX kernels.
  float4 act_div_scales = {1.f, 1 / 4.f, 1 / 16.f, 1 / 64.f};

  for (; k < K; k += k_jump) {
    // Find specific group to which channels handled by this thread
    // belong.
    uint k_block_index = k / group_size;
    uint scales_group_offset = (k_block_index * N + n);

    vecT scales =
        (reinterpret_cast<constant vecT *>(scales_ptr + scales_group_offset))[0];
    // Adding zero point results in 10% perf penalty.
    vecT zeros =
        (reinterpret_cast<constant vecT *>(zeros_ptr + scales_group_offset))[0];
    float4 zeros_float = float4(zeros);

    float4 a_val = float4(A_ptr[k / 4]);
    // We are gonna skip right-shifts of the weights and hence divide by corresponding factor.
    float4 a_vec = a_val * act_div_scales;
    float a_val_sum = a_val[0] + a_val[1] + a_val[2] + a_val[3];

    float4x4 b_mat;
    ushort b_val0 = (B_ptr + (k + 0 * K) / k_pack_factor)[0];
    ushort b_val1 = (B_ptr + (k + 1 * K) / k_pack_factor)[0];
    ushort b_val2 = (B_ptr + (k + 2 * K) / k_pack_factor)[0];
    ushort b_val3 = (B_ptr + (k + 3 * K) / k_pack_factor)[0];
    b_mat[0] = scales[0] * float4(float(b_val0 & 0x03), float(b_val0 & 0x0c),
                               float(b_val0 & 0x30), float(b_val0 & 0xc0));
    b_mat[1] = scales[1] * float4(float(b_val1 & 0x03), float(b_val1 & 0x0c),
                               float(b_val1 & 0x30), float(b_val1 & 0xc0));
    b_mat[2] = scales[2] * float4(float(b_val2 & 0x03), float(b_val2 & 0x0c),
                               float(b_val2 & 0x30), float(b_val2 & 0xc0));
    b_mat[3] = scales[3] * float4(float(b_val3 & 0x03), float(b_val3 & 0x0c),
                               float(b_val3 & 0x30), float(b_val3 & 0xc0));

    result += a_vec * b_mat;
    result += a_val_sum * zeros_float;
  }
  result += simd_shuffle_down(result, 1);
  result += simd_shuffle_down(result, 2);
  result += simd_shuffle_down(result, 4);
  result += simd_shuffle_down(result, 8);
  result += simd_shuffle_down(result, 16);
  if (tid_in_simdgroup % threads_per_channel == 0) {
    reinterpret_cast<device vecT *>(output_data + m * N)[n / 4] = vecT(result);
  }
}

#define INSTANTIATE_INT2MM(DTYPE, GSIZE)                                       \
  template [[host_name("int2pack_mm_" #GSIZE "_" #DTYPE)]] kernel void         \
  int2pack_mm<DTYPE, GSIZE>(                                                   \
      constant DTYPE * A [[buffer(0)]], constant uchar * B [[buffer(1)]],      \
      constant DTYPE * scales_ptr [[buffer(2)]],                               \
      constant DTYPE * zeros_ptr [[buffer(3)]],                                \
      device DTYPE * output_data [[buffer(4)]],                                \
      constant uint3 & sizes [[buffer(5)]],                                    \
      uint3 thread_index [[thread_position_in_grid]],                          \
      uint tid_in_simdgroup [[thread_index_in_simdgroup]])

INSTANTIATE_INT2MM(float, 32);
INSTANTIATE_INT2MM(half, 32);
INSTANTIATE_INT2MM(float, 64);
INSTANTIATE_INT2MM(half, 64);
INSTANTIATE_INT2MM(float, 128);
INSTANTIATE_INT2MM(half, 128);
INSTANTIATE_INT2MM(float, 256);
INSTANTIATE_INT2MM(half, 256);
#if __METAL_VERSION__ >= 310
INSTANTIATE_INT2MM(bfloat, 32);
INSTANTIATE_INT2MM(bfloat, 64);
INSTANTIATE_INT2MM(bfloat, 128);
INSTANTIATE_INT2MM(bfloat, 256);
#endif



/**
 * Contents of int3mm_opt.metal
 */

#include <metal_simdgroup>
#include <metal_stdlib>
using namespace metal;

inline void unpack_3bit(const uchar3 b, thread float* w) {
  w[0] = float(((b[0] & 1) << 2) | (b[1] & 3));
  w[1] = float(((b[0] & 2) << 1) | ((b[1] & 12) >> 2));
  w[2] = float((b[0] & 4) | ((b[1] & 48) >> 4));
  w[3] = float(((b[0] & 8) >> 1) | ((b[1] & 192) >> 6));

  w[4] = float(((b[0] & 16) >> 2) | (b[2] & 3));
  w[5] = float(((b[0] & 32) >> 3) | ((b[2] & 12) >> 2));
  w[6] = float(((b[0] & 64) >> 4) | ((b[2] & 48) >> 4));
  w[7] = float(((b[0] & 128) >> 5) | ((b[2] & 192) >> 6));
}

/**
 * 3-Bit Quantized Linear.
 *
 * @param[A] M x K input tensor of floating point dtype (Float, Half, BFloat16)
 * @param[B] Packed & quantized weight tensor of uint8 dtype. Expected shape is N x (3 * K / 8)
 * @param[scales] 2D tensor containg the scales for each group. Expected shape is #groups x N
 * @param[zeros] 2D tensor containg the zero points for each group. Expected shape is #groups x N
 * @param[outputData] M x N output tensor of floating point dtype (same as input)
 * @param[sizes] The sizes involved in the order: M, K, N
 *
 */
template <typename T, unsigned group_size>
kernel void int3pack_mm(constant T *A [[buffer(0)]],
                        constant uchar *B [[buffer(1)]],
                        constant T *scales_ptr [[buffer(2)]],
                        constant T *zeros_ptr [[buffer(3)]],
                        device T *output_data [[buffer(4)]],
                        constant uint3 &sizes [[buffer(5)]], // M, K, N
                        uint3 thread_index [[thread_position_in_grid]],
                        uint tid_in_simdgroup [[thread_index_in_simdgroup]]) {
  constexpr uint threads_per_channel = 32;
  constexpr uint ks_per_thread = 8;
  constexpr uint bytes_per_pack = 3;
  constexpr uint k_pack_factor = 8;
  const uint K = sizes.y;
  const uint N = sizes.z;
  uint n = thread_index.x; // 0..N/4-1
  uint m = thread_index.z; // 0..M
  n = n / threads_per_channel;
  n = n * 4;

  // This is starting k for each thread.
  uint k = (tid_in_simdgroup % threads_per_channel) * ks_per_thread;
  constexpr int k_jump = threads_per_channel * ks_per_thread;

  using vecT = typename Vec4Type<T>::type;
  constant vecT *A_ptr = reinterpret_cast<constant vecT *>(A + m * K);
  constant uchar *B_ptr = B + n * bytes_per_pack * K / k_pack_factor;

  thread float4 result = float4(0.0);

  for (; k < K; k += k_jump) {
    // Find specific group to which channels handled by this thread
    // belong.
    uint k_block_index = k / group_size;
    uint scales_group_offset = (k_block_index * N + n);

    vecT scales =
        (reinterpret_cast<constant vecT *>(scales_ptr + scales_group_offset))[0];
    vecT zeros =
        (reinterpret_cast<constant vecT *>(zeros_ptr + scales_group_offset))[0];
    float4 zeros_float = float4(zeros);

    float4 a_val[2];
    a_val[0] = float4(A_ptr[k / 4]);
    a_val[1] = float4(A_ptr[k / 4 + 1]);

    float a_val_sum = a_val[0][0] + a_val[0][1] + a_val[0][2] + a_val[0][3];
    a_val_sum += a_val[1][0] + a_val[1][1] + a_val[1][2] + a_val[1][3];

    uchar3 b_val0 = (reinterpret_cast<constant uchar3 *>(
        B_ptr + bytes_per_pack * (k + 0 * K) / k_pack_factor))[0];
    uchar3 b_val1 = (reinterpret_cast<constant uchar3 *>(
        B_ptr + bytes_per_pack * (k + 1 * K) / k_pack_factor))[0];
    uchar3 b_val2 = (reinterpret_cast<constant uchar3 *>(
        B_ptr + bytes_per_pack * (k + 2 * K) / k_pack_factor))[0];
    uchar3 b_val3 = (reinterpret_cast<constant uchar3 *>(
        B_ptr + bytes_per_pack * (k + 3 * K) / k_pack_factor))[0];

    float4x4 b_mat[2];

    thread float w0[8];
    unpack_3bit(b_val0, w0);

    thread float w1[8];
    unpack_3bit(b_val1, w1);

    thread float w2[8];
    unpack_3bit(b_val2, w2);

    thread float w3[8];
    unpack_3bit(b_val3, w3);

    b_mat[0][0] = scales[0] * float4(w0[0], w0[1], w0[2], w0[3]),
    b_mat[1][0] = scales[0] * float4(w0[4], w0[5], w0[6], w0[7]),
    b_mat[0][1] = scales[1] * float4(w1[0], w1[1], w1[2], w1[3]),
    b_mat[1][1] = scales[1] * float4(w1[4], w1[5], w1[6], w1[7]),
    b_mat[0][2] = scales[2] * float4(w2[0], w2[1], w2[2], w2[3]),
    b_mat[1][2] = scales[2] * float4(w2[4], w2[5], w2[6], w2[7]),
    b_mat[0][3] = scales[3] * float4(w3[0], w3[1], w3[2], w3[3]),
    b_mat[1][3] = scales[3] * float4(w3[4], w3[5], w3[6], w3[7]),

    result += a_val[0] * b_mat[0];
    result += a_val[1] * b_mat[1];
    result += a_val_sum * zeros_float;
  }
  result += simd_shuffle_down(result, 1);
  result += simd_shuffle_down(result, 2);
  result += simd_shuffle_down(result, 4);
  result += simd_shuffle_down(result, 8);
  result += simd_shuffle_down(result, 16);
  if (tid_in_simdgroup % threads_per_channel == 0) {
    reinterpret_cast<device vecT *>(output_data + m * N)[n / 4] = vecT(result);
  }
}

#define INSTANTIATE_INT3MM(DTYPE, GSIZE)                                       \
  template [[host_name("int3pack_mm_" #GSIZE "_" #DTYPE)]] kernel void         \
  int3pack_mm<DTYPE, GSIZE>(                                                   \
      constant DTYPE * A [[buffer(0)]], constant uchar * B [[buffer(1)]],      \
      constant DTYPE * scales_ptr [[buffer(2)]],                               \
      constant DTYPE * zeros_ptr [[buffer(3)]],                                \
      device DTYPE * output_data [[buffer(4)]],                                \
      constant uint3 & sizes [[buffer(5)]],                                    \
      uint3 thread_index [[thread_position_in_grid]],                          \
      uint tid_in_simdgroup [[thread_index_in_simdgroup]])

INSTANTIATE_INT3MM(float, 32);
INSTANTIATE_INT3MM(half, 32);
INSTANTIATE_INT3MM(float, 64);
INSTANTIATE_INT3MM(half, 64);
INSTANTIATE_INT3MM(float, 128);
INSTANTIATE_INT3MM(half, 128);
INSTANTIATE_INT3MM(float, 256);
INSTANTIATE_INT3MM(half, 256);
#if __METAL_VERSION__ >= 310
INSTANTIATE_INT3MM(bfloat, 32);
INSTANTIATE_INT3MM(bfloat, 64);
INSTANTIATE_INT3MM(bfloat, 128);
INSTANTIATE_INT3MM(bfloat, 256);
#endif



/**
 * Contents of int4mm_opt.metal
 */

#include <metal_simdgroup>
#include <metal_stdlib>
using namespace metal;

/*
   This code takes heavy inspiration from MLX:
   https://github.com/ml-explore/mlx/blob/main/mlx/backend/metal/kernels/quantized.h
   Specifically:
     - Multiplying activation by inverse scaling factor to reduce compute
   boundedness
     - Handling zero point by accumulating act in separate sum term. Needed with
   optimization done above. MLX MIT License:
   https://github.com/ml-explore/mlx/blob/main/LICENSE
*/

/*
   A matrix is [M x K] (right now this kernel does not support M > 1 but this is
   a very easy fix that will follow right after) B matrix is [N x K]. For 4 bit
   2 of the k values are packed in one byte so you can think of B as [N x K/2]
   matrix from layout perspective.

   Since this kernel is optimizing for gemv case, we split work, along reduction
   dim k, among the threads of same simdgroup. Ex: if K = 4096 and simdgroup
   size is 32 (current algorithm should work as long as simdgroup size is > 32).
   Then each thread will accumulate 4096/32 = 128 k values. However these 128
   values, handled by each thread are not laid out contiguously. Each thread
   handles 4 contiguous k values and then jumps 128 elements, k_jump =
   thread_per_channel (32) * ks_per_thread (4). Take a simpler example where
   simdgroup is of size 4. In this case threads_per_channel = 4. Assume K = 32
      k                thread
   [0, 1, 2, 3,          0
    4, 5, 6, 7,          1
    8, 9, 10, 11,        2
    12, 13, 14, 15,      3
    16, 17, 18, 19,      0
    20, 21, 22, 23,      1
    24, 25, 26, 27,      2
    28, 29, 30, 31]      3
   thread id in simd group that handle corresponding
   ks
   Thread 0 here is handling (0, 1, 2, 3) and then (16, 17, 18, 19). They are
   apart by k_jump = 4 * 4 = 16 This is done to improve memory access locality
   amonng threads that are working co-operatively. Once each thread has their
   partial sums accumulated, we use tree reduction (Metal offers simd_sum but
   not used so that we support simdgroup size = 64). In the
   example above we will have 4 partial sums.

   Each thread also handles 4 different output rows. Thus each simdgroup will be
   responsible for (1x4) tile of the output. We haven't evaluated whether a
   different tile size is better or not. We probably will do some auto-tuning
   once initial work is done.
*/

/*
   @brief This shader implements 4-bit matrix-vector multiplication where A
   matrix is fp16, bfloat or float and B matrix is a 4-bit groupwise-quantized weight
   matrix.
   @param [in] A is activation matrix of size M x K.
   @param [in] B is weight matrix of size M x K. Each byte contains 2 4-bit
   values, along K dim, packed together.
   @param [in] scales_ptr is scales ptr corresponding each
   output channel x groups. These are packed as [num_groups = ceil(K / group_size), N]. N = output
   channels.
   @param [in] zeros_ptr is zero points corresponding each
   output channel x groups. These are packed as [num_groups = ceil(K / group_size), N]. N = output
   channels.
   output channel x groups. These are packed as [num_groups = ceil(K / group_size), N, 2]. N = output
   @param [out] output_data is output matrix of size M x N.
   @param [in] sizes array contains values of M, K and N.
   @param [in] thread_index is global thread id.
   @param [in] tid_in_simdgruop is thread id in simdgroup. e.g. in simdgroup of size 32 it can be in [0-31].
*/
template <typename T, unsigned group_size>
kernel void int4pack_mm(constant T *A [[buffer(0)]],
                        constant uchar *B [[buffer(1)]],
                        constant T *scales_ptr [[buffer(2)]],
                        constant T *zeros_ptr [[buffer(3)]],
                        device T *output_data [[buffer(4)]],
                        constant uint3 &sizes [[buffer(5)]], // M, K, N
                        uint3 thread_index [[thread_position_in_grid]],
                        uint tid_in_simdgroup [[thread_index_in_simdgroup]]) {
  constexpr uint threads_per_channel = 32;
  constexpr uint ks_per_thread = 4;
  constexpr uint k_pack_factor = 2;
  const uint K = sizes.y;
  const uint N = sizes.z;
  uint n = thread_index.x; // 0..N/4-1
  uint m = thread_index.z; // 0..M
  n = n / threads_per_channel;
  n = n * 4;
  // This is starting k for each thread. In the example above, for thread 1 this
  // value will be 4.
  uint k = (tid_in_simdgroup % threads_per_channel) * ks_per_thread;
  constexpr int k_jump = threads_per_channel * ks_per_thread;

  using vecT = typename Vec4Type<T>::type;
  constant vecT *A_ptr = reinterpret_cast<constant vecT *>(A + m * K);
  constant uchar *B_ptr = B + ((n * K) / k_pack_factor);

  thread float4 result = float4(0.0);
  // We multipy group of 4 channels with these scales.
  // Because corresponding values from weight matrix are effectively left
  // shifted. This is to avoid doing right shift on those values which ends up
  // affecting performance. This is the trick applied in MLX kernels.
  float4 act_div_scales = {1.f, 1 / 16.f, 1 / 256.f, 1 / 4096.f};

  for (; k < K; k += k_jump) {
    // Find specific group to which channels handled by this thread
    // belong.
    uint k_block_index = k / group_size;
    uint scales_group_offset = (k_block_index * N + n);

    vecT scales =
        (reinterpret_cast<constant vecT *>(scales_ptr + scales_group_offset))[0];
    // Adding zero point results in 10% perf penalty.
    vecT zeros =
        (reinterpret_cast<constant vecT *>(zeros_ptr + scales_group_offset))[0];
    float4 zeros_float = float4(zeros);

    float4 a_val = float4(A_ptr[k / 4]);
    // We are gonna skip right-shifts of the weights and hence divide by corresponding factor.
    float4 a_vec = a_val * act_div_scales;
    float a_val_sum = a_val[0] + a_val[1] + a_val[2] + a_val[3];

    float4x4 b_mat;
    ushort b_val0 = (reinterpret_cast<constant ushort *>(
        B_ptr + (k + 0 * K) / k_pack_factor))[0];
    ushort b_val1 = (reinterpret_cast<constant ushort *>(
        B_ptr + (k + 1 * K) / k_pack_factor))[0];
    ushort b_val2 = (reinterpret_cast<constant ushort *>(
        B_ptr + (k + 2 * K) / k_pack_factor))[0];
    ushort b_val3 = (reinterpret_cast<constant ushort *>(
        B_ptr + (k + 3 * K) / k_pack_factor))[0];
    b_mat[0] = scales[0] * float4(float(b_val0 & 0x000f), float(b_val0 & 0x00f0),
                               float(b_val0 & 0x0f00), float(b_val0 & 0xf000));
    b_mat[1] = scales[1] * float4(float(b_val1 & 0x000f), float(b_val1 & 0x00f0),
                               float(b_val1 & 0x0f00), float(b_val1 & 0xf000));
    b_mat[2] = scales[2] * float4(float(b_val2 & 0x000f), float(b_val2 & 0x00f0),
                               float(b_val2 & 0x0f00), float(b_val2 & 0xf000));
    b_mat[3] = scales[3] * float4(float(b_val3 & 0x000f), float(b_val3 & 0x00f0),
                               float(b_val3 & 0x0f00), float(b_val3 & 0xf000));

    result += a_vec * b_mat;
    result += a_val_sum * zeros_float;
  }
  result += simd_shuffle_down(result, 1);
  result += simd_shuffle_down(result, 2);
  result += simd_shuffle_down(result, 4);
  result += simd_shuffle_down(result, 8);
  result += simd_shuffle_down(result, 16);
  if (tid_in_simdgroup % threads_per_channel == 0) {
    reinterpret_cast<device vecT *>(output_data + m * N)[n / 4] = vecT(result);
  }
}

#define INSTANTIATE_INT4MM(DTYPE, GSIZE)                                       \
  template [[host_name("int4pack_mm_" #GSIZE "_" #DTYPE)]] kernel void         \
  int4pack_mm<DTYPE, GSIZE>(                                                   \
      constant DTYPE * A [[buffer(0)]], constant uchar * B [[buffer(1)]],      \
      constant DTYPE * scales_ptr [[buffer(2)]],                               \
      constant DTYPE * zeros_ptr [[buffer(3)]],                                \
      device DTYPE * output_data [[buffer(4)]],                                \
      constant uint3 & sizes [[buffer(5)]],                                    \
      uint3 thread_index [[thread_position_in_grid]],                          \
      uint tid_in_simdgroup [[thread_index_in_simdgroup]])

INSTANTIATE_INT4MM(float, 32);
INSTANTIATE_INT4MM(half, 32);
INSTANTIATE_INT4MM(float, 64);
INSTANTIATE_INT4MM(half, 64);
INSTANTIATE_INT4MM(float, 128);
INSTANTIATE_INT4MM(half, 128);
INSTANTIATE_INT4MM(float, 256);
INSTANTIATE_INT4MM(half, 256);
#if __METAL_VERSION__ >= 310
INSTANTIATE_INT4MM(bfloat, 32);
INSTANTIATE_INT4MM(bfloat, 64);
INSTANTIATE_INT4MM(bfloat, 128);
INSTANTIATE_INT4MM(bfloat, 256);
#endif



/**
 * Contents of int5mm.metal
 */

#include <metal_stdlib>
using namespace metal;

/**
 * 5-Bit Quantized Linear.
 *
 * @param[A] M x K input tensor of floating point dtype (Float, Half, BFloat16)
 * @param[B] Packed & quantized weight tensor of uint8 dtype. Expected shape is N x (5 * K / 8)
 * @param[scales] 2D tensor containg the scales for each group. Expected shape is #groups x N
 * @param[zeros] 2D tensor containg the zero points for each group. Expected shape is #groups x N
 * @param[outputData] M x N output tensor of floating point dtype (same as input)
 * @param[sizes] The sizes involved in the order: M, K, N
 *
 * Dispatched threads: N x M x 1
 */
template<typename T, unsigned groupSize>
kernel void int5pack_mm(
    constant T                 * A              [[buffer(0)]],
    constant uchar             * B              [[buffer(1)]],
    constant T                 * scales         [[buffer(2)]],
    constant T                 * zeros          [[buffer(3)]],
    device   T                 * outputData     [[buffer(4)]],
    constant uint3             & sizes          [[buffer(5)]], // M, K, N
    uint2                        thread_index   [[thread_position_in_grid]]) {
    const uint K = sizes.y;
    const uint N = sizes.z;
    const uint m = thread_index.y; // 0..M-1
    const uint n = thread_index.x; // 0..N-1
    const uint32_t k_block = (K + groupSize - 1) / groupSize;
    constant T *A_ptr = A + m * K;
    constant uchar *B_ptr = B + n * 5 * K / 8;

    float rc = 0.0;
    uint k = 0;
    for (uint32_t kb = 0; kb < k_block ; kb ++) {
      const float scale = float(scales[kb * N + n]);
      const float zero = float(zeros[kb * N + n]);
      for(uint idx = 0; idx < groupSize && k < K; idx+=8, k+=8) {
        const auto a_val0 = float(A_ptr[k + 0]);
        const auto a_val1 = float(A_ptr[k + 1]);
        const auto a_val2 = float(A_ptr[k + 2]);
        const auto a_val3 = float(A_ptr[k + 3]);
        const auto a_val4 = float(A_ptr[k + 4]);
        const auto a_val5 = float(A_ptr[k + 5]);
        const auto a_val6 = float(A_ptr[k + 6]);
        const auto a_val7 = float(A_ptr[k + 7]);

        uchar b0 = B_ptr[5 * (k / 8) + 0];
        uchar b1 = B_ptr[5 * (k / 8) + 1];
        uchar b2 = B_ptr[5 * (k / 8) + 2];
        uchar b3 = B_ptr[5 * (k / 8) + 3];
        uchar b4 = B_ptr[5 * (k / 8) + 4];

        uchar w_val0 = ((b0 & 1) << 4) | (b1 & 15);
        uchar w_val1 = ((b0 & 2) << 3) | ((b1 & 240) >> 4);
        uchar w_val2 = ((b0 & 4) << 2) | (b2 & 15);
        uchar w_val3 = ((b0 & 8) << 1) | ((b2 & 240) >> 4);

        uchar w_val4 = ((b0 & 16))       | (b3 & 15);
        uchar w_val5 = ((b0 & 32) >> 1)  | ((b3 & 240) >> 4);
        uchar w_val6 = ((b0 & 64) >> 2)  | (b4 & 15);
        uchar w_val7 = ((b0 & 128) >> 3) | ((b4 & 240) >> 4);

        rc += a_val0 * (scale * float(w_val0) + zero);
        rc += a_val1 * (scale * float(w_val1) + zero);
        rc += a_val2 * (scale * float(w_val2) + zero);
        rc += a_val3 * (scale * float(w_val3) + zero);
        rc += a_val4 * (scale * float(w_val4) + zero);
        rc += a_val5 * (scale * float(w_val5) + zero);
        rc += a_val6 * (scale * float(w_val6) + zero);
        rc += a_val7 * (scale * float(w_val7) + zero);
      }
    }
    outputData[m * N + n] = T(rc);
}

#define INSTANTIATE_INT5MM(DTYPE, GSIZE)                                 \
template                                                                 \
[[host_name("int5pack_mm_" #GSIZE "_" #DTYPE)]]                          \
kernel void int5pack_mm<DTYPE, GSIZE>(                                   \
    constant DTYPE             * A              [[buffer(0)]],           \
    constant uchar             * B              [[buffer(1)]],           \
    constant DTYPE             * scales         [[buffer(2)]],           \
    constant DTYPE             * zeros          [[buffer(3)]],           \
    device   DTYPE             * outputData     [[buffer(4)]],           \
    constant uint3             & sizes          [[buffer(5)]],           \
    uint2                        thread_index [[thread_position_in_grid]])

INSTANTIATE_INT5MM(float, 32);
INSTANTIATE_INT5MM(half, 32);
INSTANTIATE_INT5MM(float, 64);
INSTANTIATE_INT5MM(half, 64);
INSTANTIATE_INT5MM(float, 128);
INSTANTIATE_INT5MM(half, 128);
INSTANTIATE_INT5MM(float, 256);
INSTANTIATE_INT5MM(half, 256);
#if __METAL_VERSION__ >= 310
INSTANTIATE_INT5MM(bfloat, 32);
INSTANTIATE_INT5MM(bfloat, 64);
INSTANTIATE_INT5MM(bfloat, 128);
INSTANTIATE_INT5MM(bfloat, 256);
#endif



/**
 * Contents of int6mm.metal
 */

#include <metal_stdlib>
using namespace metal;

/**
 * 6-Bit Quantized Linear.
 *
 * @param[A] M x K input tensor of floating point dtype (Float, Half, BFloat16)
 * @param[B] Packed & quantized weight tensor of uint8 dtype. Expected shape is N x (6 * K / 8)
 * @param[scales] 2D tensor containg the scales for each group. Expected shape is #groups x N
 * @param[zeros] 2D tensor containg the zero points for each group. Expected shape is #groups x N
 * @param[outputData] M x N output tensor of floating point dtype (same as input)
 * @param[sizes] The sizes involved in the order: M, K, N
 *
 * Dispatched threads: N x M x 1
 */
template<typename T, unsigned groupSize>
kernel void int6pack_mm(
    constant T                 * A              [[buffer(0)]],
    constant uchar             * B              [[buffer(1)]],
    constant T                 * scales         [[buffer(2)]],
    constant T                 * zeros          [[buffer(3)]],
    device   T                 * outputData     [[buffer(4)]],
    constant uint3             & sizes          [[buffer(5)]], // M, K, N
    uint2                        thread_index   [[thread_position_in_grid]]) {
    const uint K = sizes.y;
    const uint N = sizes.z;
    const uint m = thread_index.y; // 0..M-1
    const uint n = thread_index.x; // 0..N-1
    const uint32_t k_block = (K + groupSize - 1) / groupSize;
    constant T *A_ptr = A + m * K;
    constant uchar *B_ptr = B + n * 3 * K / 4;

    float rc = 0.0;
    uint k = 0;
    for (uint32_t kb = 0; kb < k_block ; kb ++) {
      const float scale = float(scales[kb * N + n]);
      const float zero = float(zeros[kb * N + n]);
      for(uint idx = 0; idx < groupSize && k < K; idx+=8, k+=8) {
        const auto a_val0 = float(A_ptr[k + 0]);
        const auto a_val1 = float(A_ptr[k + 1]);
        const auto a_val2 = float(A_ptr[k + 2]);
        const auto a_val3 = float(A_ptr[k + 3]);

        const auto a_val4 = float(A_ptr[k + 4]);
        const auto a_val5 = float(A_ptr[k + 5]);
        const auto a_val6 = float(A_ptr[k + 6]);
        const auto a_val7 = float(A_ptr[k + 7]);

        uchar b0 = B_ptr[3 * (k / 4) + 0];
        uchar b1 = B_ptr[3 * (k / 4) + 1];
        uchar b2 = B_ptr[3 * (k / 4) + 2];

        uchar b3 = B_ptr[3 * (k / 4) + 3];
        uchar b4 = B_ptr[3 * (k / 4) + 4];
        uchar b5 = B_ptr[3 * (k / 4) + 5];

        uchar w_val0 = ((b0 & 3) << 4) | (b1 & 15);
        uchar w_val1 = ((b0 & 12) << 2) | ((b1 & 240) >> 4);
        uchar w_val2 = ((b0 & 48)) | (b2 & 15);
        uchar w_val3 = ((b0 & 192) >> 2) | ((b2 & 240) >> 4);

        uchar w_val4 = ((b3 & 3) << 4) | (b4 & 15);
        uchar w_val5 = ((b3 & 12) << 2) | ((b4 & 240) >> 4);
        uchar w_val6 = ((b3 & 48)) | (b5 & 15);
        uchar w_val7 = ((b3 & 192) >> 2) | ((b5 & 240) >> 4);

        rc += a_val0 * (scale * float(w_val0) + zero);
        rc += a_val1 * (scale * float(w_val1) + zero);
        rc += a_val2 * (scale * float(w_val2) + zero);
        rc += a_val3 * (scale * float(w_val3) + zero);

        rc += a_val4 * (scale * float(w_val4) + zero);
        rc += a_val5 * (scale * float(w_val5) + zero);
        rc += a_val6 * (scale * float(w_val6) + zero);
        rc += a_val7 * (scale * float(w_val7) + zero);
      }
    }
    outputData[m * N + n] = T(rc);
}

#define INSTANTIATE_INT6MM(DTYPE, GSIZE)                                 \
template                                                                 \
[[host_name("int6pack_mm_" #GSIZE "_" #DTYPE)]]                          \
kernel void int6pack_mm<DTYPE, GSIZE>(                                   \
    constant DTYPE             * A              [[buffer(0)]],           \
    constant uchar             * B              [[buffer(1)]],           \
    constant DTYPE             * scales         [[buffer(2)]],           \
    constant DTYPE             * zeros          [[buffer(3)]],           \
    device   DTYPE             * outputData     [[buffer(4)]],           \
    constant uint3             & sizes          [[buffer(5)]],           \
    uint2                        thread_index [[thread_position_in_grid]])

INSTANTIATE_INT6MM(float, 32);
INSTANTIATE_INT6MM(half, 32);
INSTANTIATE_INT6MM(float, 64);
INSTANTIATE_INT6MM(half, 64);
INSTANTIATE_INT6MM(float, 128);
INSTANTIATE_INT6MM(half, 128);
INSTANTIATE_INT6MM(float, 256);
INSTANTIATE_INT6MM(half, 256);
#if __METAL_VERSION__ >= 310
INSTANTIATE_INT6MM(bfloat, 32);
INSTANTIATE_INT6MM(bfloat, 64);
INSTANTIATE_INT6MM(bfloat, 128);
INSTANTIATE_INT6MM(bfloat, 256);
#endif



/**
 * Contents of int7mm.metal
 */

#include <metal_stdlib>
using namespace metal;

/**
 * 7-Bit Quantized Linear.
 *
 * @param[A] M x K input tensor of floating point dtype (Float, Half, BFloat16)
 * @param[B] Packed & quantized weight tensor of uint8 dtype. Expected shape is N x (7 * K / 8)
 * @param[scales] 2D tensor containg the scales for each group. Expected shape is #groups x N
 * @param[zeros] 2D tensor containg the zero points for each group. Expected shape is #groups x N
 * @param[outputData] M x N output tensor of floating point dtype (same as input)
 * @param[sizes] The sizes involved in the order: M, K, N
 *
 * Dispatched threads: N x M x 1
 */
template<typename T, unsigned groupSize>
kernel void int7pack_mm(
    constant T                 * A              [[buffer(0)]],
    constant uchar             * B              [[buffer(1)]],
    constant T                 * scales         [[buffer(2)]],
    constant T                 * zeros          [[buffer(3)]],
    device   T                 * outputData     [[buffer(4)]],
    constant uint3             & sizes          [[buffer(5)]], // M, K, N
    uint2                        thread_index   [[thread_position_in_grid]]) {
    const uint K = sizes.y;
    const uint N = sizes.z;
    const uint m = thread_index.y; // 0..M-1
    const uint n = thread_index.x; // 0..N-1
    const uint32_t k_block = (K + groupSize - 1) / groupSize;
    constant T *A_ptr = A + m * K;
    constant uchar *B_ptr = B + n * 7 * K / 8;

    float rc = 0.0;
    uint k = 0;
    for (uint32_t kb = 0; kb < k_block ; kb ++) {
      const float scale = float(scales[kb * N + n]);
      const float zero = float(zeros[kb * N + n]);
      for(uint idx = 0; idx < groupSize && k < K; idx+=8, k+=8) {
        const auto a_val0 = float(A_ptr[k + 0]);
        const auto a_val1 = float(A_ptr[k + 1]);
        const auto a_val2 = float(A_ptr[k + 2]);
        const auto a_val3 = float(A_ptr[k + 3]);
        const auto a_val4 = float(A_ptr[k + 4]);
        const auto a_val5 = float(A_ptr[k + 5]);
        const auto a_val6 = float(A_ptr[k + 6]);
        const auto a_val7 = float(A_ptr[k + 7]);

        uchar b0 = B_ptr[7 * (k / 8) + 0];
        uchar b1 = B_ptr[7 * (k / 8) + 1];
        uchar b2 = B_ptr[7 * (k / 8) + 2];
        uchar b3 = B_ptr[7 * (k / 8) + 3];
        uchar b4 = B_ptr[7 * (k / 8) + 4];
        uchar b5 = B_ptr[7 * (k / 8) + 5];
        uchar b6 = B_ptr[7 * (k / 8) + 6];

        uchar w_val0 = b0 & 127;
        uchar w_val1 = b1 & 127;
        uchar w_val2 = b2 & 127;
        uchar w_val3 = b3 & 127;
        uchar w_val4 = b4 & 127;
        uchar w_val5 = b5 & 127;
        uchar w_val6 = b6 & 127;
        uchar w_val7 = ((b0 & 128) >> 7) | ((b1 & 128) >> 6) | ((b2 & 128) >> 5) | ((b3 & 128) >> 4)
          | ((b4 & 128) >> 3) | ((b5 & 128) >> 2) | ((b6 & 128) >> 1);

        rc += a_val0 * (scale * float(w_val0) + zero);
        rc += a_val1 * (scale * float(w_val1) + zero);
        rc += a_val2 * (scale * float(w_val2) + zero);
        rc += a_val3 * (scale * float(w_val3) + zero);
        rc += a_val4 * (scale * float(w_val4) + zero);
        rc += a_val5 * (scale * float(w_val5) + zero);
        rc += a_val6 * (scale * float(w_val6) + zero);
        rc += a_val7 * (scale * float(w_val7) + zero);
      }
    }
    outputData[m * N + n] = T(rc);
}

#define INSTANTIATE_INT7MM(DTYPE, GSIZE)                                 \
template                                                                 \
[[host_name("int7pack_mm_" #GSIZE "_" #DTYPE)]]                          \
kernel void int7pack_mm<DTYPE, GSIZE>(                                   \
    constant DTYPE             * A              [[buffer(0)]],           \
    constant uchar             * B              [[buffer(1)]],           \
    constant DTYPE             * scales         [[buffer(2)]],           \
    constant DTYPE             * zeros          [[buffer(3)]],           \
    device   DTYPE             * outputData     [[buffer(4)]],           \
    constant uint3             & sizes          [[buffer(5)]],           \
    uint2                        thread_index [[thread_position_in_grid]])

INSTANTIATE_INT7MM(float, 32);
INSTANTIATE_INT7MM(half, 32);
INSTANTIATE_INT7MM(float, 64);
INSTANTIATE_INT7MM(half, 64);
INSTANTIATE_INT7MM(float, 128);
INSTANTIATE_INT7MM(half, 128);
INSTANTIATE_INT7MM(float, 256);
INSTANTIATE_INT7MM(half, 256);
#if __METAL_VERSION__ >= 310
INSTANTIATE_INT7MM(bfloat, 32);
INSTANTIATE_INT7MM(bfloat, 64);
INSTANTIATE_INT7MM(bfloat, 128);
INSTANTIATE_INT7MM(bfloat, 256);
#endif



)METAL_LOWBIT");
